Appropriate data for making a predictive model has requirements. For example, it needs to be labeled, and it needs to have proper features.
The dataset I'm using here is a collision dataset provided by SPD and recorded by Traffic Records. This dataset keeps track of the road accident in Seattle from 2004 till present. This dataset, which is updated weekly, has almost 200 thousand rows and 38 columns. Our target/dependent variable is one of those columns and is called SEVERITY. Concretely each sample/row will have 37 independent variables and include the weather situation, the quality/quantity of the vehicles and pedestrians involved. However, not all of these attributes are used for training the model. For example, the intersection unique key (denoted INTKEY in the Dataset) could be helpful if we are using this model to predict collision severity in Seattle but not if we are using it to predict collision severity in NewYork.
Some of the features are totally unnecessary for our project, for example, the OBJECTID and SHAPE will not be helpful in training.
Some features require moderate and some require complex processing of the data. For example, in this  project, we will drop the severity Description column (denoted SEVERITYDESC)  and the LOCATION column because these columns contain humans natural language text, and cannot be translated into computer language(i.e. vectors of numbers) without NLP machine learning technique called sentiment analysis.
Finally, the features that do not have any description/metadata, and don't show meaningful correlation with the target variable, will be dropped.
